{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A simple neural network and cost function gradient via jax"
      ],
      "metadata": {
        "id": "OAaBOgjUps0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/FlorianMarquardt/machine-learning-for-physicists/blob/master/2024/03_NN_CostFunctionGradientFromJax.ipynb)"
      ],
      "metadata": {
        "id": "wR3pRGrCrq6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example code for the lecture series \"Machine Learning for Physicists\" by Florian Marquardt\n",
        "\n",
        "Lecture 3\n",
        "\n",
        "See https://machine-learning-for-physicists.org and the current course website linked there!\n",
        "\n",
        "This notebook shows how to build a little network and evaluate the gradient of the cost function using jax, and how to apply one step of gradient descent.\n",
        "\n",
        "MIT License."
      ],
      "metadata": {
        "id": "KGjabluhpdBH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CFxxh7Y0g1yr"
      },
      "outputs": [],
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple function, to show the principles of jax grad:\n",
        "def f(x):\n",
        "  return jnp.sum(x**2)"
      ],
      "metadata": {
        "id": "Y2H0vWx8qy0w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use jax autodifferentiation to get gradient of f\n",
        "# with respect to the input vector x\n",
        "grad_f=grad(f)"
      ],
      "metadata": {
        "id": "t9kGPerBq3zU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# show what happens in a numerical example:\n",
        "x=jnp.array([0.1,0.2,0.3])\n",
        "\n",
        "print(\"value of f: \", f(x))\n",
        "print(\"value of grad f:\", grad_f(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcvvmGL3q6vT",
        "outputId": "80a232d6-8647-4e97-bbcf-4af692c67423"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "value of f:  0.14000002\n",
            "value of grad f: [0.2 0.4 0.6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now define a simple neural network\n",
        "# (arbitrary number of layers, but relu activation at each layer)\n",
        "def network(parameters,x):\n",
        "  \"\"\"\n",
        "  Evaluate network.\n",
        "\n",
        "  parameters=[[weights1,biases1],[weights2,biases2],...]\n",
        "  x=input vector\n",
        "  \"\"\"\n",
        "  for weights,biases in parameters:\n",
        "    # weights has shape (neurons_lower_layer,neurons_upper_layer),\n",
        "    # biases has shape (neurons_upper_layer,)\n",
        "    z=jnp.dot(x,weights)+biases\n",
        "    x=(z>0)*z # relu activation\n",
        "  return x"
      ],
      "metadata": {
        "id": "U-95MFmAg6A3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our network has structure 2 (input) -- 3 -- 1 (output)\n",
        "weights1=jnp.array([[0.1,0.3,0.5],[-0.4,0.2,0.8]]) # shape (2,3)\n",
        "biases1=jnp.array([0.1,-0.2,0.3]) # shape (3,)\n",
        "weights2=jnp.array([[0.2],[0.7],[-0.5]]) # shape (3,1)\n",
        "biases2=jnp.array([0.2]) # shape (1,)\n",
        "\n",
        "params=[[weights1,biases1],[weights2,biases2]]"
      ],
      "metadata": {
        "id": "seLhYn-Ch-xc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# apply network to test input\n",
        "x=jnp.array([0.3,-0.5])\n",
        "print(network(params,x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PK5p7hfjLN_",
        "outputId": "677e6e3d-b0f3-43c1-f931-ba7c4104bee0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.241]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a cost function (here: quadratic deviation)\n",
        "def cost(params,x,y_target):\n",
        "  return jnp.sum( ( network(params,x) - y_target )**2 )\n",
        "# note: we would divide by the batch size jnp.shape(x)[0] if we want to average\n",
        "# over a batch (but right now we do not do batches)"
      ],
      "metadata": {
        "id": "CcDF6tb8kb0R"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost(params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuWFKRvClNHU",
        "outputId": "b0ba05df-afc5-4481-d75b-e63c1f9a9b6a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(0.576081, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now apply jax autodifferentiation to get the\n",
        "# gradient of the cost function with respect to the params:\n",
        "grad_cost=grad(cost,argnums=0) # argnums=0 means first argument, i.e. params"
      ],
      "metadata": {
        "id": "d6BH6ti5jbSD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params"
      ],
      "metadata": {
        "id": "LA__XRagi0Qw",
        "outputId": "0fc3705c-a2e5-4b57-c70b-4f2078e6bba7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Array([[ 0.1,  0.3,  0.5],\n",
              "         [-0.4,  0.2,  0.8]], dtype=float32),\n",
              "  Array([ 0.1, -0.2,  0.3], dtype=float32)],\n",
              " [Array([[ 0.2],\n",
              "         [ 0.7],\n",
              "         [-0.5]], dtype=float32),\n",
              "  Array([0.2], dtype=float32)]]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the gradient of the cost function,\n",
        "# at the current values of the parameters 'params',\n",
        "# and for that given x input vector (and with y_target==1.0):\n",
        "grad_cost(params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kH9MHw2XkV5L",
        "outputId": "ad1af232-9c70-4610-d4ff-db0d01fde451"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Array([[-0.09108001, -0.        ,  0.22770001],\n",
              "         [ 0.1518    ,  0.        , -0.3795    ]], dtype=float32),\n",
              "  Array([-0.3036, -0.    ,  0.759 ], dtype=float32)],\n",
              " [Array([[-0.50094   ],\n",
              "         [-0.        ],\n",
              "         [-0.07590002]], dtype=float32),\n",
              "  Array([-1.518], dtype=float32)]]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This output is just the same shape as params!\n",
        "Each component is the gradient of the cost function with respect to that component!"
      ],
      "metadata": {
        "id": "t5tMeWOopSeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store this whole nested list:\n",
        "grad_value = grad_cost(params,x,1.0)"
      ],
      "metadata": {
        "id": "pzEr1eCJlhSj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now update all parameters according to the negative gradient!\n",
        "learning_rate=0.1\n",
        "\n",
        "new_params=[(weights-learning_rate*grad_weights,biases-learning_rate*grad_biases) for\n",
        "            (weights,biases),(grad_weights,grad_biases) in zip(params,grad_value)]"
      ],
      "metadata": {
        "id": "DNNZ3m3Tlopi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print new parameters:\n",
        "new_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPxZr1_YmVQl",
        "outputId": "cdb97eba-942f-4914-adf0-8a8e85a4b961"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Array([[ 0.109108,  0.3     ,  0.47723 ],\n",
              "         [-0.41518 ,  0.2     ,  0.83795 ]], dtype=float32),\n",
              "  Array([ 0.13036   , -0.2       ,  0.22410001], dtype=float32)),\n",
              " (Array([[ 0.250094],\n",
              "         [ 0.7     ],\n",
              "         [-0.49241 ]], dtype=float32),\n",
              "  Array([0.35180002], dtype=float32))]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cost should have gone down!\n",
        "cost(new_params,x,1.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3d0QQW2mSwz",
        "outputId": "680724b8-3547-4260-9fdd-55df6fcf68d7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(0.3085742, dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# jax makes such things more convenient:\n",
        "from jax.tree_util import tree_map\n",
        "\n",
        "new_params = tree_map(lambda x,y: x - learning_rate * y, params, grad_value)"
      ],
      "metadata": {
        "id": "hQrINMERmazO"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# should be the same result as above:\n",
        "new_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2n8iHVGTnD4Z",
        "outputId": "2ccd1062-c220-4753-8213-753685859322"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[Array([[ 0.109108,  0.3     ,  0.47723 ],\n",
              "         [-0.41518 ,  0.2     ,  0.83795 ]], dtype=float32),\n",
              "  Array([ 0.13036   , -0.2       ,  0.22410001], dtype=float32)],\n",
              " [Array([[ 0.250094],\n",
              "         [ 0.7     ],\n",
              "         [-0.49241 ]], dtype=float32),\n",
              "  Array([0.35180002], dtype=float32)]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train neural networks using jax and simple\n",
        "stochastic gradient descent! Have fun!\n",
        "\n",
        "(no flax or optax or anything else needed at this point!)"
      ],
      "metadata": {
        "id": "difGqMG4pO9C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CsA2CZaEnEpp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}